{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPICNsh7o6YPcjzcqqodfKz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyeshaAreej/NLPMSCS2-Practce/blob/main/NLPMSCS2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict, Counter\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "3tTS6prRmJH2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1ee9c5e-1cde-4813-a88b-2b86cefe2384"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs = [\n",
        "    (\"This is a great day\", \"positive\"),\n",
        "    (\"This is a bad day\", \"negative\"),\n",
        "    (\"I love the beautiful weather today\", \"positive\"),\n",
        "    (\"I am so happy with my results\", \"positive\"),\n",
        "    (\"The food at this restaurant is amazing\", \"positive\"),\n",
        "    (\"The service was terrible and slow\", \"negative\"),\n",
        "    (\"I hate when my phone battery dies\", \"negative\"),\n",
        "    (\"I feel fantastic after my workout\", \"positive\"),\n",
        "    (\"The traffic jam made me late for work\", \"negative\"),\n",
        "    (\"It was a wonderful experience\", \"positive\"),\n",
        "    (\"I am feeling very sick today\", \"negative\"),\n",
        "    (\"The movie was absolutely brilliant\", \"positive\"),\n",
        "    (\"The flight delay was frustrating\", \"negative\"),\n",
        "    (\"I am so excited for my vacation\", \"positive\"),\n",
        "    (\"The customer service was awful\", \"negative\"),\n",
        "    (\"The view from the mountain was breathtaking\", \"positive\"),\n",
        "    (\"I am disappointed with the product quality\", \"negative\"),\n",
        "    (\"My friends always support me\", \"positive\"),\n",
        "    (\"The noise in the neighborhood is unbearable\", \"negative\"),\n",
        "    (\"I had a fantastic weekend\", \"positive\")\n",
        "]\n"
      ],
      "metadata": {
        "id": "4uW7c3ICmTmK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for Positive and Negative Counts\n",
        "vocabulary = defaultdict(lambda: {\"positive\": 0, \"negative\": 0})\n",
        "for text, label in paragraphs:\n",
        "  tokens = word_tokenize(text.lower())\n",
        "  for token in tokens:\n",
        "    vocabulary[token][label] += 1\n",
        "\n",
        "# Print the vocabulary in a readable format\n",
        "print(\"\\nWord Frequency Table:\")\n",
        "print(\"=\" * 30)\n",
        "print(f\"{'Word':<15} {'Positive':<10} {'Negative':<10}\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "for word, freq in sorted(vocabulary.items()):\n",
        "    print(f\"{word:<15} {freq['positive']:<10} {freq['negative']:<10}\")\n",
        "\n",
        "print(\"=\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3SUnNPrk3aC",
        "outputId": "26f7cdc8-c8cd-4633-dc3e-0130f9ceac55"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Frequency Table:\n",
            "==============================\n",
            "Word            Positive   Negative  \n",
            "==============================\n",
            "a               3          1         \n",
            "absolutely      1          0         \n",
            "after           1          0         \n",
            "always          1          0         \n",
            "am              2          2         \n",
            "amazing         1          0         \n",
            "and             0          1         \n",
            "at              1          0         \n",
            "awful           0          1         \n",
            "bad             0          1         \n",
            "battery         0          1         \n",
            "beautiful       1          0         \n",
            "breathtaking    1          0         \n",
            "brilliant       1          0         \n",
            "customer        0          1         \n",
            "day             1          1         \n",
            "delay           0          1         \n",
            "dies            0          1         \n",
            "disappointed    0          1         \n",
            "excited         1          0         \n",
            "experience      1          0         \n",
            "fantastic       2          0         \n",
            "feel            1          0         \n",
            "feeling         0          1         \n",
            "flight          0          1         \n",
            "food            1          0         \n",
            "for             1          1         \n",
            "friends         1          0         \n",
            "from            1          0         \n",
            "frustrating     0          1         \n",
            "great           1          0         \n",
            "had             1          0         \n",
            "happy           1          0         \n",
            "hate            0          1         \n",
            "i               5          3         \n",
            "in              0          1         \n",
            "is              2          2         \n",
            "it              1          0         \n",
            "jam             0          1         \n",
            "late            0          1         \n",
            "love            1          0         \n",
            "made            0          1         \n",
            "me              1          1         \n",
            "mountain        1          0         \n",
            "movie           1          0         \n",
            "my              4          1         \n",
            "neighborhood    0          1         \n",
            "noise           0          1         \n",
            "phone           0          1         \n",
            "product         0          1         \n",
            "quality         0          1         \n",
            "restaurant      1          0         \n",
            "results         1          0         \n",
            "service         0          2         \n",
            "sick            0          1         \n",
            "slow            0          1         \n",
            "so              2          0         \n",
            "support         1          0         \n",
            "terrible        0          1         \n",
            "the             5          7         \n",
            "this            2          1         \n",
            "today           1          1         \n",
            "traffic         0          1         \n",
            "unbearable      0          1         \n",
            "vacation        1          0         \n",
            "very            0          1         \n",
            "view            1          0         \n",
            "was             3          3         \n",
            "weather         1          0         \n",
            "weekend         1          0         \n",
            "when            0          1         \n",
            "with            1          1         \n",
            "wonderful       1          0         \n",
            "work            0          1         \n",
            "workout         1          0         \n",
            "==============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Extraction Code\n",
        "\n",
        "# Generate feature vectors for each sentence\n",
        "print(\"\\nFeature Vectors for Each Sentence:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "feature_vectors = []\n",
        "\n",
        "for text, _ in paragraphs:\n",
        "    positive_sum = 0\n",
        "    negative_sum = 0\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    for token in tokens:\n",
        "        if token in vocabulary:\n",
        "            positive_sum += vocabulary[token][\"positive\"]\n",
        "            negative_sum += vocabulary[token][\"negative\"]\n",
        "\n",
        "    feature_vector = [1, positive_sum, negative_sum]\n",
        "    feature_vectors.append((text, feature_vector))\n",
        "\n",
        "\n",
        "    print(f\"Sentence: {text}\")\n",
        "    print(f\"Feature Vector: {feature_vector}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Debugging: Print sentiment sums for all text data\n",
        "print(f\"\\nTotal Positive Score: {positive_sum}\")\n",
        "print(f\"Total Negative Score: {negative_sum}\")\n",
        "\n",
        "# Debugging: Print feature vector\n",
        "print(f\"\\nFeature Vector: {feature_vector}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmJ5z1AhpZJg",
        "outputId": "a9b9cfdd-7007-4e2f-d8cd-b1079a6b57cc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature Vectors for Each Sentence:\n",
            "==================================================\n",
            "Sentence: This is a great day\n",
            "Feature Vector: [1, 9, 5]\n",
            "--------------------------------------------------\n",
            "Sentence: This is a bad day\n",
            "Feature Vector: [1, 8, 6]\n",
            "--------------------------------------------------\n",
            "Sentence: I love the beautiful weather today\n",
            "Feature Vector: [1, 14, 11]\n",
            "--------------------------------------------------\n",
            "Sentence: I am so happy with my results\n",
            "Feature Vector: [1, 16, 7]\n",
            "--------------------------------------------------\n",
            "Sentence: The food at this restaurant is amazing\n",
            "Feature Vector: [1, 13, 10]\n",
            "--------------------------------------------------\n",
            "Sentence: The service was terrible and slow\n",
            "Feature Vector: [1, 8, 15]\n",
            "--------------------------------------------------\n",
            "Sentence: I hate when my phone battery dies\n",
            "Feature Vector: [1, 9, 9]\n",
            "--------------------------------------------------\n",
            "Sentence: I feel fantastic after my workout\n",
            "Feature Vector: [1, 14, 4]\n",
            "--------------------------------------------------\n",
            "Sentence: The traffic jam made me late for work\n",
            "Feature Vector: [1, 7, 14]\n",
            "--------------------------------------------------\n",
            "Sentence: It was a wonderful experience\n",
            "Feature Vector: [1, 9, 4]\n",
            "--------------------------------------------------\n",
            "Sentence: I am feeling very sick today\n",
            "Feature Vector: [1, 8, 9]\n",
            "--------------------------------------------------\n",
            "Sentence: The movie was absolutely brilliant\n",
            "Feature Vector: [1, 11, 10]\n",
            "--------------------------------------------------\n",
            "Sentence: The flight delay was frustrating\n",
            "Feature Vector: [1, 8, 13]\n",
            "--------------------------------------------------\n",
            "Sentence: I am so excited for my vacation\n",
            "Feature Vector: [1, 16, 7]\n",
            "--------------------------------------------------\n",
            "Sentence: The customer service was awful\n",
            "Feature Vector: [1, 8, 14]\n",
            "--------------------------------------------------\n",
            "Sentence: The view from the mountain was breathtaking\n",
            "Feature Vector: [1, 17, 17]\n",
            "--------------------------------------------------\n",
            "Sentence: I am disappointed with the product quality\n",
            "Feature Vector: [1, 13, 16]\n",
            "--------------------------------------------------\n",
            "Sentence: My friends always support me\n",
            "Feature Vector: [1, 8, 2]\n",
            "--------------------------------------------------\n",
            "Sentence: The noise in the neighborhood is unbearable\n",
            "Feature Vector: [1, 12, 20]\n",
            "--------------------------------------------------\n",
            "Sentence: I had a fantastic weekend\n",
            "Feature Vector: [1, 12, 4]\n",
            "--------------------------------------------------\n",
            "\n",
            "Total Positive Score: 12\n",
            "Total Negative Score: 4\n",
            "\n",
            "Feature Vector: [1, 12, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuOrWA8ZkG0r",
        "outputId": "87eaf3bf-098e-4963-feef-c8a5fa16c1e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['learning', 'NLP', 'exciting']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#stopwords and punctuation\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Example text\n",
        "text = \"I am learning NLP, and it is very exciting!\"\n",
        "\n",
        "# Get English stop words\n",
        "stop_w = set(stopwords.words('english'))\n",
        "\n",
        "# Tokenize words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Remove stop words and punctuation\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_w and word not in string.punctuation]\n",
        "\n",
        "print(filtered_tokens)  # Output: ['learning', 'NLP', 'exciting']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmitization\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Ensure required resources are available\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Sample sentences\n",
        "sentences = [\n",
        "    \"The cars are running fast on the roads.\",\n",
        "    \"He is thinking about his studies.\",\n",
        "    \"The leaves have fallen from the trees.\",\n",
        "    \"She enjoys being alone.\",\n",
        "    \"The birds are flying in the sky.\",\n",
        "    \"Children are playing happily in the park.\",\n",
        "]\n",
        "\n",
        "# Initialize Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"\\nLemmatization Results\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for sentence in sentences:\n",
        "    # Tokenize sentence into words\n",
        "    tokens = word_tokenize(sentence.lower())\n",
        "\n",
        "    # Apply lemmatization\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, wordnet.VERB) for word in tokens]\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nSentence: {sentence}\")\n",
        "    print(f\"{'Word':<15} {'Lemmatized':<15}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    for word, lemma in zip(tokens, lemmatized_words):\n",
        "        print(f\"{word:<15} {lemma:<15}\")\n",
        "\n",
        "    print(\"=\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAVPQ9lYqN1H",
        "outputId": "7ad1fbd5-6f1d-4af0-ed2e-597b95bf7ce8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatization Results\n",
            "==================================================\n",
            "\n",
            "Sentence: The cars are running fast on the roads.\n",
            "Word            Lemmatized     \n",
            "------------------------------\n",
            "the             the            \n",
            "cars            cars           \n",
            "are             be             \n",
            "running         run            \n",
            "fast            fast           \n",
            "on              on             \n",
            "the             the            \n",
            "roads           roads          \n",
            ".               .              \n",
            "==================================================\n",
            "\n",
            "Sentence: He is thinking about his studies.\n",
            "Word            Lemmatized     \n",
            "------------------------------\n",
            "he              he             \n",
            "is              be             \n",
            "thinking        think          \n",
            "about           about          \n",
            "his             his            \n",
            "studies         study          \n",
            ".               .              \n",
            "==================================================\n",
            "\n",
            "Sentence: The leaves have fallen from the trees.\n",
            "Word            Lemmatized     \n",
            "------------------------------\n",
            "the             the            \n",
            "leaves          leave          \n",
            "have            have           \n",
            "fallen          fall           \n",
            "from            from           \n",
            "the             the            \n",
            "trees           tree           \n",
            ".               .              \n",
            "==================================================\n",
            "\n",
            "Sentence: She enjoys being alone.\n",
            "Word            Lemmatized     \n",
            "------------------------------\n",
            "she             she            \n",
            "enjoys          enjoy          \n",
            "being           be             \n",
            "alone           alone          \n",
            ".               .              \n",
            "==================================================\n",
            "\n",
            "Sentence: The birds are flying in the sky.\n",
            "Word            Lemmatized     \n",
            "------------------------------\n",
            "the             the            \n",
            "birds           bird           \n",
            "are             be             \n",
            "flying          fly            \n",
            "in              in             \n",
            "the             the            \n",
            "sky             sky            \n",
            ".               .              \n",
            "==================================================\n",
            "\n",
            "Sentence: Children are playing happily in the park.\n",
            "Word            Lemmatized     \n",
            "------------------------------\n",
            "children        children       \n",
            "are             be             \n",
            "playing         play           \n",
            "happily         happily        \n",
            "in              in             \n",
            "the             the            \n",
            "park            park           \n",
            ".               .              \n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}